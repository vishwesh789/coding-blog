<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Object Detection with YOLOv5: A Step-by-Step Guide</title><meta name="description" content="Object detection is a fundamental task in computer vision, and it has numerous applications such as autonomous driving, video surveillance, and image search. With the recent advances in deep learning, object detection has become easier and more accurate than ever before. In this guide, we will introduce you to YOLOv5, one of the most popular object detection algorithms, and walk you through the steps of training your own object detection model."/><meta property="og:url" content="https://www.codewithgolu.com/python/object-detection-with-yol-ov5-a-step-by-step-guide/"/><meta property="og:type" content="website"/><meta property="og:title" content="Object Detection with YOLOv5: A Step-by-Step Guide"/><meta property="og:description" content="Object detection is a fundamental task in computer vision, and it has numerous applications such as autonomous driving, video surveillance, and image search. With the recent advances in deep learning, object detection has become easier and more accurate than ever before. In this guide, we will introduce you to YOLOv5, one of the most popular object detection algorithms, and walk you through the steps of training your own object detection model."/><meta property="og:image" content="https://images.pexels.com/photos/574070/pexels-photo-574070.jpeg"/><meta name="twitter:card" content="summary_large_image"/><meta property="twitter:domain" content="codewithgolu.com"/><meta property="twitter:url" content="https://www.codewithgolu.com/python/object-detection-with-yol-ov5-a-step-by-step-guide/"/><meta name="twitter:title" content="Object Detection with YOLOv5: A Step-by-Step Guide"/><meta name="twitter:description" content="Object detection is a fundamental task in computer vision, and it has numerous applications such as autonomous driving, video surveillance, and image search. With the recent advances in deep learning, object detection has become easier and more accurate than ever before. In this guide, we will introduce you to YOLOv5, one of the most popular object detection algorithms, and walk you through the steps of training your own object detection model."/><meta name="twitter:image" content="https://images.pexels.com/photos/574070/pexels-photo-574070.jpeg"/><meta name="next-head-count" content="15"/><link rel="icon" href="/favicon.svg"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><link rel="preload" href="/_next/static/css/198502990ad5deb6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/198502990ad5deb6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-114634acb84f8baa.js" defer=""></script><script src="/_next/static/chunks/main-ba528792e922dabc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-f7e120030d214ba2.js" defer=""></script><script src="/_next/static/chunks/247-b06725ee5bb02744.js" defer=""></script><script src="/_next/static/chunks/338-ddaf447963f864b9.js" defer=""></script><script src="/_next/static/chunks/945-9df356926dbc7595.js" defer=""></script><script src="/_next/static/chunks/71-350b7e4015ced14e.js" defer=""></script><script src="/_next/static/chunks/pages/python/%5Bpython%5D-538e6c071f291026.js" defer=""></script><script src="/_next/static/VEuKj9rENWBlSP1WmJ69e/_buildManifest.js" defer=""></script><script src="/_next/static/VEuKj9rENWBlSP1WmJ69e/_ssgManifest.js" defer=""></script></head><body id="top"><div id="__next"><main><div><div><header class="header" data-header="true" id="top"><div class="container"><a class="logo" href="/"><img alt="codewithgolu" src="/images/healthWealthLogo.png" width="80" height="25" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a><nav class="navbar" data-navbar="true"><div class="navbar-top"><a class="logo" href="/"><img alt="codewithgolu" src="/images/healthWealthLogo.png" width="80" height="25" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a><button class="nav-close-btn" aria-label="close menu" data-nav-toggler="true"><ion-icon name="close-outline" aria-hidden="true"></ion-icon></button></div><ul class="navbar-list"><li><a class="navbar-link hover-1" data-nav-toggler="true" href="/category/python/">Python</a></li><li><a class="navbar-link hover-1" data-nav-toggler="true" href="/category/reactjs/">ReactJs</a></li><li><a class="navbar-link hover-1" data-nav-toggler="true" href="/category/node-js/">Node Js</a></li></ul><p class="copyright-text" style="margin-top:20px">Copyright 2022 Â© codewithgolu -Developed by Vishwesh Singh</p></nav><a class="btn btn-primary" href="/python/object-detection-with-yol-ov5-a-step-by-step-guide/#">Subscribe</a><button class="nav-open-btn" aria-label="open menu" data-nav-toggler="true"><ion-icon name="menu-outline" aria-hidden="true"></ion-icon></button></div></header><a class="back-top-btn" aria-label="back to top" data-back-top-btn="true" href="/python/object-detection-with-yol-ov5-a-step-by-step-guide/#top"><ion-icon name="arrow-up-outline" aria-hidden="true"></ion-icon></a></div><div class="container" style="margin-top:120px"><div class="card feature-card"><figure class="card-banner img-holder" style="--width:1602;--height:903"><img alt="Object Detection with YOLOv5: A Step-by-Step Guide" src="https://images.pexels.com/photos/574070/pexels-photo-574070.jpeg" width="1602" height="903" decoding="async" data-nimg="1" class="img-cover" loading="lazy" style="color:transparent"/></figure><div style="flex-direction:row;display:flex;gap:5px"><button aria-label="facebook" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#3b5998"></circle><path d="M34.1,47V33.3h4.6l0.7-5.3h-5.3v-3.4c0-1.5,0.4-2.6,2.6-2.6l2.8,0v-4.8c-0.5-0.1-2.2-0.2-4.1-0.2 c-4.1,0-6.9,2.5-6.9,7V28H24v5.3h4.6V47H34.1z" fill="white"></path></svg></button><button aria-label="pinterest" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#cb2128"></circle><path d="M32,16c-8.8,0-16,7.2-16,16c0,6.6,3.9,12.2,9.6,14.7c0-1.1,0-2.5,0.3-3.7 c0.3-1.3,2.1-8.7,2.1-8.7s-0.5-1-0.5-2.5c0-2.4,1.4-4.1,3.1-4.1c1.5,0,2.2,1.1,2.2,2.4c0,1.5-0.9,3.7-1.4,5.7 c-0.4,1.7,0.9,3.1,2.5,3.1c3,0,5.1-3.9,5.1-8.5c0-3.5-2.4-6.1-6.7-6.1c-4.9,0-7.9,3.6-7.9,7.7c0,1.4,0.4,2.4,1.1,3.1 c0.3,0.3,0.3,0.5,0.2,0.9c-0.1,0.3-0.3,1-0.3,1.3c-0.1,0.4-0.4,0.6-0.8,0.4c-2.2-0.9-3.3-3.4-3.3-6.1c0-4.5,3.8-10,11.4-10 c6.1,0,10.1,4.4,10.1,9.2c0,6.3-3.5,11-8.6,11c-1.7,0-3.4-0.9-3.9-2c0,0-0.9,3.7-1.1,4.4c-0.3,1.2-1,2.5-1.6,3.4 c1.4,0.4,3,0.7,4.5,0.7c8.8,0,16-7.2,16-16C48,23.2,40.8,16,32,16z" fill="white"></path></svg></button><button aria-label="reddit" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#ff4500"></circle><path d="m 52.8165,31.942362 c 0,-2.4803 -2.0264,-4.4965 -4.5169,-4.4965 -1.2155,0 -2.3171,0.4862 -3.128,1.2682 -3.077,-2.0247 -7.2403,-3.3133 -11.8507,-3.4782 l 2.5211,-7.9373 6.8272,1.5997 -0.0102,0.0986 c 0,2.0281 1.6575,3.6771 3.6958,3.6771 2.0366,0 3.6924,-1.649 3.6924,-3.6771 0,-2.0281 -1.6575,-3.6788 -3.6924,-3.6788 -1.564,0 -2.8968,0.9758 -3.4357,2.3443 l -7.3593,-1.7255 c -0.3213,-0.0782 -0.6477,0.1071 -0.748,0.4233 L 32,25.212062 c -4.8246,0.0578 -9.1953,1.3566 -12.41,3.4425 -0.8058,-0.7446 -1.8751,-1.2104 -3.0583,-1.2104 -2.4905,0 -4.5152,2.0179 -4.5152,4.4982 0,1.649 0.9061,3.0787 2.2389,3.8607 -0.0884,0.4794 -0.1462,0.9639 -0.1462,1.4569 0,6.6487 8.1736,12.0581 18.2223,12.0581 10.0487,0 18.224,-5.4094 18.224,-12.0581 0,-0.4658 -0.0493,-0.9248 -0.1275,-1.377 1.4144,-0.7599 2.3885,-2.2304 2.3885,-3.9406 z m -29.2808,3.0872 c 0,-1.4756 1.207,-2.6775 2.6894,-2.6775 1.4824,0 2.6877,1.2019 2.6877,2.6775 0,1.4756 -1.2053,2.6758 -2.6877,2.6758 -1.4824,0 -2.6894,-1.2002 -2.6894,-2.6758 z m 15.4037,7.9373 c -1.3549,1.3481 -3.4816,2.0043 -6.5008,2.0043 l -0.0221,-0.0051 -0.0221,0.0051 c -3.0209,0 -5.1476,-0.6562 -6.5008,-2.0043 -0.2465,-0.2448 -0.2465,-0.6443 0,-0.8891 0.2465,-0.2465 0.6477,-0.2465 0.8942,0 1.105,1.0999 2.9393,1.6337 5.6066,1.6337 l 0.0221,0.0051 0.0221,-0.0051 c 2.6673,0 4.5016,-0.5355 5.6066,-1.6354 0.2465,-0.2465 0.6477,-0.2448 0.8942,0 0.2465,0.2465 0.2465,0.6443 0,0.8908 z m -0.3213,-5.2615 c -1.4824,0 -2.6877,-1.2002 -2.6877,-2.6758 0,-1.4756 1.2053,-2.6775 2.6877,-2.6775 1.4824,0 2.6877,1.2019 2.6877,2.6775 0,1.4756 -1.2053,2.6758 -2.6877,2.6758 z" fill="white"></path></svg></button><button aria-label="whatsapp" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#25D366"></circle><path d="m42.32286,33.93287c-0.5178,-0.2589 -3.04726,-1.49644 -3.52105,-1.66732c-0.4712,-0.17346 -0.81554,-0.2589 -1.15987,0.2589c-0.34175,0.51004 -1.33075,1.66474 -1.63108,2.00648c-0.30032,0.33658 -0.60064,0.36247 -1.11327,0.12945c-0.5178,-0.2589 -2.17994,-0.80259 -4.14759,-2.56312c-1.53269,-1.37217 -2.56312,-3.05503 -2.86603,-3.57283c-0.30033,-0.5178 -0.03366,-0.80259 0.22524,-1.06149c0.23301,-0.23301 0.5178,-0.59547 0.7767,-0.90616c0.25372,-0.31068 0.33657,-0.5178 0.51262,-0.85437c0.17088,-0.36246 0.08544,-0.64725 -0.04402,-0.90615c-0.12945,-0.2589 -1.15987,-2.79613 -1.58964,-3.80584c-0.41424,-1.00971 -0.84142,-0.88027 -1.15987,-0.88027c-0.29773,-0.02588 -0.64208,-0.02588 -0.98382,-0.02588c-0.34693,0 -0.90616,0.12945 -1.37736,0.62136c-0.4712,0.5178 -1.80194,1.76053 -1.80194,4.27186c0,2.51134 1.84596,4.945 2.10227,5.30747c0.2589,0.33657 3.63497,5.51458 8.80262,7.74113c1.23237,0.5178 2.1903,0.82848 2.94111,1.08738c1.23237,0.38836 2.35599,0.33657 3.24402,0.20712c0.99159,-0.15534 3.04985,-1.24272 3.47963,-2.45956c0.44013,-1.21683 0.44013,-2.22654 0.31068,-2.45955c-0.12945,-0.23301 -0.46601,-0.36247 -0.98382,-0.59548m-9.40068,12.84407l-0.02589,0c-3.05503,0 -6.08417,-0.82849 -8.72495,-2.38189l-0.62136,-0.37023l-6.47252,1.68286l1.73463,-6.29129l-0.41424,-0.64725c-1.70875,-2.71846 -2.6149,-5.85116 -2.6149,-9.07706c0,-9.39809 7.68934,-17.06155 17.15993,-17.06155c4.58253,0 8.88029,1.78642 12.11655,5.02268c3.23625,3.21036 5.02267,7.50812 5.02267,12.06476c-0.0078,9.3981 -7.69712,17.06155 -17.14699,17.06155m14.58906,-31.58846c-3.93529,-3.80584 -9.1133,-5.95471 -14.62789,-5.95471c-11.36055,0 -20.60848,9.2065 -20.61625,20.52564c0,3.61684 0.94757,7.14565 2.75211,10.26282l-2.92557,10.63564l10.93337,-2.85309c3.0136,1.63108 6.4052,2.4958 9.85634,2.49839l0.01037,0c11.36574,0 20.61884,-9.2091 20.62403,-20.53082c0,-5.48093 -2.14111,-10.64081 -6.03239,-14.51915" fill="white"></path></svg></button><button aria-label="linkedin" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#007fb1"></circle><path d="M20.4,44h5.4V26.6h-5.4V44z M23.1,18c-1.7,0-3.1,1.4-3.1,3.1c0,1.7,1.4,3.1,3.1,3.1 c1.7,0,3.1-1.4,3.1-3.1C26.2,19.4,24.8,18,23.1,18z M39.5,26.2c-2.6,0-4.4,1.4-5.1,2.8h-0.1v-2.4h-5.2V44h5.4v-8.6 c0-2.3,0.4-4.5,3.2-4.5c2.8,0,2.8,2.6,2.8,4.6V44H46v-9.5C46,29.8,45,26.2,39.5,26.2z" fill="white"></path></svg></button><button aria-label="telegram" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#37aee2"></circle><path d="m45.90873,15.44335c-0.6901,-0.0281 -1.37668,0.14048 -1.96142,0.41265c-0.84989,0.32661 -8.63939,3.33986 -16.5237,6.39174c-3.9685,1.53296 -7.93349,3.06593 -10.98537,4.24067c-3.05012,1.1765 -5.34694,2.05098 -5.4681,2.09312c-0.80775,0.28096 -1.89996,0.63566 -2.82712,1.72788c-0.23354,0.27218 -0.46884,0.62161 -0.58825,1.10275c-0.11941,0.48114 -0.06673,1.09222 0.16682,1.5716c0.46533,0.96052 1.25376,1.35737 2.18443,1.71383c3.09051,0.99037 6.28638,1.93508 8.93263,2.8236c0.97632,3.44171 1.91401,6.89571 2.84116,10.34268c0.30554,0.69185 0.97105,0.94823 1.65764,0.95525l-0.00351,0.03512c0,0 0.53908,0.05268 1.06412,-0.07375c0.52679,-0.12292 1.18879,-0.42846 1.79109,-0.99212c0.662,-0.62161 2.45836,-2.38812 3.47683,-3.38552l7.6736,5.66477l0.06146,0.03512c0,0 0.84989,0.59703 2.09312,0.68132c0.62161,0.04214 1.4399,-0.07726 2.14229,-0.59176c0.70766,-0.51626 1.1765,-1.34683 1.396,-2.29506c0.65673,-2.86224 5.00979,-23.57745 5.75257,-27.00686l-0.02107,0.08077c0.51977,-1.93157 0.32837,-3.70159 -0.87096,-4.74991c-0.60054,-0.52152 -1.2924,-0.7498 -1.98425,-0.77965l0,0.00176zm-0.2072,3.29069c0.04741,0.0439 0.0439,0.0439 0.00351,0.04741c-0.01229,-0.00351 0.14048,0.2072 -0.15804,1.32576l-0.01229,0.04214l-0.00878,0.03863c-0.75858,3.50668 -5.15554,24.40802 -5.74203,26.96472c-0.08077,0.34417 -0.11414,0.31959 -0.09482,0.29852c-0.1756,-0.02634 -0.50045,-0.16506 -0.52679,-0.1756l-13.13468,-9.70175c4.4988,-4.33199 9.09945,-8.25307 13.744,-12.43229c0.8218,-0.41265 0.68483,-1.68573 -0.29852,-1.70681c-1.04305,0.24584 -1.92279,0.99564 -2.8798,1.47502c-5.49971,3.2626 -11.11882,6.13186 -16.55882,9.49279c-2.792,-0.97105 -5.57873,-1.77704 -8.15298,-2.57601c2.2336,-0.89555 4.00889,-1.55579 5.75608,-2.23009c3.05188,-1.1765 7.01687,-2.7042 10.98537,-4.24067c7.94051,-3.06944 15.92667,-6.16346 16.62028,-6.43037l0.05619,-0.02283l0.05268,-0.02283c0.19316,-0.0878 0.30378,-0.09658 0.35471,-0.10009c0,0 -0.01756,-0.05795 -0.00351,-0.04566l-0.00176,0zm-20.91715,22.0638l2.16687,1.60145c-0.93418,0.91311 -1.81743,1.77353 -2.45485,2.38812l0.28798,-3.98957" fill="white"></path></svg></button><button aria-label="twitter" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg viewBox="0 0 64 64" width="32" height="32"><circle cx="32" cy="32" r="31" fill="#00aced"></circle><path d="M48,22.1c-1.2,0.5-2.4,0.9-3.8,1c1.4-0.8,2.4-2.1,2.9-3.6c-1.3,0.8-2.7,1.3-4.2,1.6 C41.7,19.8,40,19,38.2,19c-3.6,0-6.6,2.9-6.6,6.6c0,0.5,0.1,1,0.2,1.5c-5.5-0.3-10.3-2.9-13.5-6.9c-0.6,1-0.9,2.1-0.9,3.3 c0,2.3,1.2,4.3,2.9,5.5c-1.1,0-2.1-0.3-3-0.8c0,0,0,0.1,0,0.1c0,3.2,2.3,5.8,5.3,6.4c-0.6,0.1-1.1,0.2-1.7,0.2c-0.4,0-0.8,0-1.2-0.1 c0.8,2.6,3.3,4.5,6.1,4.6c-2.2,1.8-5.1,2.8-8.2,2.8c-0.5,0-1.1,0-1.6-0.1c2.9,1.9,6.4,2.9,10.1,2.9c12.1,0,18.7-10,18.7-18.7 c0-0.3,0-0.6,0-0.8C46,24.5,47.1,23.4,48,22.1z" fill="white"></path></svg></button></div><div class="card-content"><div class="card-wrapper"><div class="card-tag"><a class="span hover-2" href="/python/object-detection-with-yol-ov5-a-step-by-step-guide/#">#<!-- -->python</a></div><div class="wrapper"><ion-icon name="time-outline" aria-hidden="true"></ion-icon><span class="span">5<!-- --> mins read</span></div></div><div class="card-wrapper"><div class="profile-card"><div><p class="card-title">By:<!-- --> <!-- -->vishwesh</p></div></div></div><h1 class="headline headline-3" style="font-size:30px">Object Detection with YOLOv5: A Step-by-Step Guide</h1><div class="content" style="margin-top:30px;word-spacing:3px;font-size:18px;font-weight:normal"><p>Object detection is a fundamental task in computer vision, and it has numerous applications such as autonomous driving, video surveillance, and image search. With the recent advances in deep learning, object detection has become easier and more accurate than ever before. In this guide, we will introduce you to YOLOv5, one of the most popular object detection algorithms, and walk you through the steps of training your own object detection model.</p>
<h2><strong>What is YOLOv5?</strong></h2>
<p>YOLOv5 is the latest version of the You Only Look Once (YOLO) algorithm family, which is a real-time object detection system. The YOLOv5 algorithm is based on a single deep neural network and can detect objects with high accuracy and speed. YOLOv5 is an improvement over the previous versions of YOLO in terms of accuracy and speed, and it is also more flexible and easier to use.</p>
<h2><strong>Installing YOLOv5</strong></h2>
<p>Before we can start using YOLOv5, we need to install it on our machine. YOLOv5 can be installed on Windows, Linux, and macOS, and it requires Python 3.8 or later. The easiest way to install YOLOv5 is by using pip, the Python package installer. To install YOLOv5, open a terminal window and type the following command:</p>
<pre class="language-plaintext"><code class="language-plaintext"><span class="code-line">pip install yolov5
</span></code></pre>
<p>This command will install the YOLOv5 package and all its dependencies.</p>
<h2><strong>Preparing the Dataset</strong></h2>
<p>The first step in training an object detection model is to prepare the dataset. The dataset should contain images of the objects we want to detect, and each image should be annotated with the bounding boxes of the objects. There are several tools available for annotating images, such as LabelImg and VGG Image Annotator (VIA).</p>
<p>Once the images are annotated, we need to split the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune the hyperparameters of the model, and the test set is used to evaluate the performance of the model.</p>
<h2><strong>Creating the Configuration File</strong></h2>
<p>After preparing the dataset, we need to create a configuration file for the YOLOv5 model. The configuration file specifies the architecture of the model, the dataset, and the training parameters. The configuration file is written in YAML format and can be edited using any text editor.</p>
<p>Here is an example of a configuration file:</p>
<pre class="language-plaintext"><code class="language-plaintext"><span class="code-line"># YOLOv5 configuration file
</span><span class="code-line">model:
</span><span class="code-line">  # Model architecture
</span><span class="code-line">  architecture: yolov5s
</span><span class="code-line">  # Number of classes
</span><span class="code-line">  num_classes: 5
</span><span class="code-line">
</span><span class="code-line">train:
</span><span class="code-line">  # Path to the training dataset
</span><span class="code-line">  train_dataset: ./data/train.yaml
</span><span class="code-line">  # Path to the validation dataset
</span><span class="code-line">  val_dataset: ./data/val.yaml
</span><span class="code-line">  # Number of epochs
</span><span class="code-line">  epochs: 50
</span><span class="code-line">  # Batch size
</span><span class="code-line">  batch_size: 16
</span><span class="code-line">  # Learning rate
</span><span class="code-line">  lr: 0.001
</span><span class="code-line">
</span><span class="code-line">test:
</span><span class="code-line">  # Path to the test dataset
</span><span class="code-line">  test_dataset: ./data/test.yaml
</span><span class="code-line">
</span></code></pre>
<p>In this example, we are using the yolov5s architecture, which is the smallest version of YOLOv5. We are training the model to detect 5 classes, and we are using a training dataset and a validation dataset. We are training the model for 50 epochs with a batch size of 16, and a learning rate of 0.001. We also have a test dataset for evaluating the performance of the model.</p>
<h2><strong>Training the Model</strong></h2>
<p>Once we have prepared the dataset and created the configuration file, we can train the model. To train the model, we need to run the following command in the terminal:</p>
<pre class="language-plaintext"><code class="language-plaintext"><span class="code-line">python train.py --img 640 --batch 16 --epochs 50 --data path/to/data.yaml --cfg path/to/model.yaml --weights yolov5s.pt --name my_experiment
</span></code></pre>
<p>In this command, we are specifying the image size, batch size, number of epochs, path to the data configuration file, path to the model configuration file, path to the pre-trained weights, and the name of the experiment. The pre-trained weights are used to initialize the model, and they can be downloaded from the YOLOv5 repository.</p>
<p>During the training process, the model will learn to detect the objects in the images and optimize its parameters to minimize the loss function. The loss function measures the difference between the predicted bounding boxes and the ground truth bounding boxes.</p>
<h2><strong>Evaluating the Model</strong></h2>
<p>After training the model, we can evaluate its performance on the test set. To evaluate the model, we need to run the following command in the terminal:</p>
<pre class="language-plaintext"><code class="language-plaintext"><span class="code-line">python test.py --weights runs/exp/my_experiment/weights/best.pt --data path/to/data.yaml --img-size 640
</span></code></pre>
<p>In this command, we are specifying the path to the trained weights, path to the data configuration file, and the image size. The best weights are selected based on the performance on the validation set, and they are saved in the <strong>runs/exp/my_experiment/weights/</strong> directory.</p>
<p>The test script will generate a set of metrics such as precision, recall, and mAP (mean Average Precision) that measure the performance of the model.</p>
<h2><strong>Using the Model for Inference</strong></h2>
<p>Once we have trained the model and evaluated its performance, we can use it for object detection on new images. To do this, we need to run the following command in the terminal:</p>
<pre class="language-plaintext"><code class="language-plaintext"><span class="code-line">python detect.py --weights runs/exp/my_experiment/weights/best.pt --img-size 640 --source path/to/images --save-txt
</span></code></pre>
<p>In this command, we are specifying the path to the trained weights, image size, path to the images, and the option to save the results as text files. The detect script will detect the objects in the images and save the results in a text file for each image.</p>
<h2><strong>Conclusion</strong></h2>
<p>In this guide, we introduced you to YOLOv5, one of the most popular object detection algorithms, and walked you through the steps of training your own object detection model. We covered the installation of YOLOv5, preparation of the dataset, creation of the configuration file, training of the model, evaluation of the model, and using the model for inference. With this guide, you should now have a good understanding of how to use YOLOv5 for object detection.</p></div></div></div></div><section class="section recent-post" id="recent" aria-labelledby="recent-label"><div class="container"><div class="post-main"><h2 class="headline headline-2 section-title"><span class="span">Recent posts</span></h2><p class="section-text">Don&#x27;t miss the latest trends</p><ul class="grid-list"></ul></div><div class="post-aside grid-list"><div class="card aside-card"><h3 class="headline headline-2 aside-title"><span class="span">Popular Posts</span></h3></div></div></div></section><section class="tags" aria-labelledby="tag-label" style="margin-top:50px"><div class="container"><h2 class="headline headline-2 section-title" id="tag-label"><span class="span">Popular Categories</span></h2><ul class="grid-list" style="margin-top:50px"><li><a class="card tag-btn" href="https://www.codewithgolu.com/category/career/"><p class="btn-text">Career</p></a></li><li><a class="card tag-btn" href="https://www.codewithgolu.com/category/finance/"><p class="btn-text">Finance</p></a></li><li><a class="card tag-btn" href="https://www.codewithgolu.com/category/lifestyle/"><p class="btn-text">Lifestyle</p></a></li><li><a class="card tag-btn" href="https://www.codewithgolu.com/category/technology/"><p class="btn-text">Technologies</p></a></li><li><a class="card tag-btn" href="https://www.codewithgolu.com/category/health/"><p class="btn-text">Health</p></a></li></ul></div></section><footer><div class="container"><div class="card footer"><div class="section footer-top"><div class="footer-brand"><a class="logo" href="/python/object-detection-with-yol-ov5-a-step-by-step-guide/#"><img alt="codewithgolu" src="/images/healthWealthLogo.png" width="80" height="25" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a><p class="footer-text">Copyright 2023 Â© codewithgolu -Developed by Vishwesh Singh.Our website&#x27;s services, content, and products are provided solely for educational purposes.</p><p class="footer-list-title">Address</p><address class="footer-text address">a-58, sector 21 <br/>Noida, 201304</address></div><div class="footer-list"><p class="footer-list-title">Newsletter</p><p class="footer-text">Sign up to be first to receive the latest stories inspiring us, case studies, and industry news.</p><div class="input-wrapper"><input type="text" name="name" placeholder="Your name" required="" class="input-field" autoComplete="off"/><ion-icon name="person-outline" aria-hidden="true"></ion-icon></div><div class="input-wrapper"><input type="email" name="email_address" placeholder="Emaill address" required="" class="input-field" autoComplete="off"/><ion-icon name="mail-outline" aria-hidden="true"></ion-icon></div><a class="btn btn-primary" href="/python/object-detection-with-yol-ov5-a-step-by-step-guide/#"><span class="span">Subscribe</span><ion-icon name="arrow-forward" aria-hidden="true"></ion-icon></a></div></div><div class="footer-bottom"><p class="copyright">Â© Developed by<!-- --> <a class="copyright-link" href="/python/object-detection-with-yol-ov5-a-step-by-step-guide/#">vishwesh</a></p></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h2: \"h2\",\n    strong: \"strong\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Object detection is a fundamental task in computer vision, and it has numerous applications such as autonomous driving, video surveillance, and image search. With the recent advances in deep learning, object detection has become easier and more accurate than ever before. In this guide, we will introduce you to YOLOv5, one of the most popular object detection algorithms, and walk you through the steps of training your own object detection model.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"What is YOLOv5?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"YOLOv5 is the latest version of the You Only Look Once (YOLO) algorithm family, which is a real-time object detection system. The YOLOv5 algorithm is based on a single deep neural network and can detect objects with high accuracy and speed. YOLOv5 is an improvement over the previous versions of YOLO in terms of accuracy and speed, and it is also more flexible and easier to use.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Installing YOLOv5\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Before we can start using YOLOv5, we need to install it on our machine. YOLOv5 can be installed on Windows, Linux, and macOS, and it requires Python 3.8 or later. The easiest way to install YOLOv5 is by using pip, the Python package installer. To install YOLOv5, open a terminal window and type the following command:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      className: \"language-plaintext\",\n      children: _jsx(_components.code, {\n        className: \"language-plaintext\",\n        children: _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"pip install yolov5\\n\"\n        })\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This command will install the YOLOv5 package and all its dependencies.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Preparing the Dataset\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The first step in training an object detection model is to prepare the dataset. The dataset should contain images of the objects we want to detect, and each image should be annotated with the bounding boxes of the objects. There are several tools available for annotating images, such as LabelImg and VGG Image Annotator (VIA).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once the images are annotated, we need to split the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune the hyperparameters of the model, and the test set is used to evaluate the performance of the model.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Creating the Configuration File\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"After preparing the dataset, we need to create a configuration file for the YOLOv5 model. The configuration file specifies the architecture of the model, the dataset, and the training parameters. The configuration file is written in YAML format and can be edited using any text editor.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Here is an example of a configuration file:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      className: \"language-plaintext\",\n      children: _jsxs(_components.code, {\n        className: \"language-plaintext\",\n        children: [_jsx(_components.span, {\n          className: \"code-line\",\n          children: \"# YOLOv5 configuration file\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"model:\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Model architecture\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  architecture: yolov5s\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Number of classes\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  num_classes: 5\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"train:\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Path to the training dataset\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  train_dataset: ./data/train.yaml\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Path to the validation dataset\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  val_dataset: ./data/val.yaml\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Number of epochs\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  epochs: 50\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Batch size\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  batch_size: 16\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Learning rate\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  lr: 0.001\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"test:\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  # Path to the test dataset\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"  test_dataset: ./data/test.yaml\\n\"\n        }), _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"\\n\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this example, we are using the yolov5s architecture, which is the smallest version of YOLOv5. We are training the model to detect 5 classes, and we are using a training dataset and a validation dataset. We are training the model for 50 epochs with a batch size of 16, and a learning rate of 0.001. We also have a test dataset for evaluating the performance of the model.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Training the Model\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once we have prepared the dataset and created the configuration file, we can train the model. To train the model, we need to run the following command in the terminal:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      className: \"language-plaintext\",\n      children: _jsx(_components.code, {\n        className: \"language-plaintext\",\n        children: _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"python train.py --img 640 --batch 16 --epochs 50 --data path/to/data.yaml --cfg path/to/model.yaml --weights yolov5s.pt --name my_experiment\\n\"\n        })\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this command, we are specifying the image size, batch size, number of epochs, path to the data configuration file, path to the model configuration file, path to the pre-trained weights, and the name of the experiment. The pre-trained weights are used to initialize the model, and they can be downloaded from the YOLOv5 repository.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"During the training process, the model will learn to detect the objects in the images and optimize its parameters to minimize the loss function. The loss function measures the difference between the predicted bounding boxes and the ground truth bounding boxes.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Evaluating the Model\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"After training the model, we can evaluate its performance on the test set. To evaluate the model, we need to run the following command in the terminal:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      className: \"language-plaintext\",\n      children: _jsx(_components.code, {\n        className: \"language-plaintext\",\n        children: _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"python test.py --weights runs/exp/my_experiment/weights/best.pt --data path/to/data.yaml --img-size 640\\n\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"In this command, we are specifying the path to the trained weights, path to the data configuration file, and the image size. The best weights are selected based on the performance on the validation set, and they are saved in the \", _jsx(_components.strong, {\n        children: \"runs/exp/my_experiment/weights/\"\n      }), \" directory.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The test script will generate a set of metrics such as precision, recall, and mAP (mean Average Precision) that measure the performance of the model.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Using the Model for Inference\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once we have trained the model and evaluated its performance, we can use it for object detection on new images. To do this, we need to run the following command in the terminal:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      className: \"language-plaintext\",\n      children: _jsx(_components.code, {\n        className: \"language-plaintext\",\n        children: _jsx(_components.span, {\n          className: \"code-line\",\n          children: \"python detect.py --weights runs/exp/my_experiment/weights/best.pt --img-size 640 --source path/to/images --save-txt\\n\"\n        })\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this command, we are specifying the path to the trained weights, image size, path to the images, and the option to save the results as text files. The detect script will detect the objects in the images and save the results in a text file for each image.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Conclusion\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this guide, we introduced you to YOLOv5, one of the most popular object detection algorithms, and walked you through the steps of training your own object detection model. We covered the installation of YOLOv5, preparation of the dataset, creation of the configuration file, training of the model, evaluation of the model, and using the model for inference. With this guide, you should now have a good understanding of how to use YOLOv5 for object detection.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}},"title":"Object Detection with YOLOv5: A Step-by-Step Guide","metaDesc":"Object detection is a fundamental task in computer vision, and it has numerous applications such as autonomous driving, video surveillance, and image search. With the recent advances in deep learning, object detection has become easier and more accurate than ever before. In this guide, we will introduce you to YOLOv5, one of the most popular object detection algorithms, and walk you through the steps of training your own object detection model.","tags":["python"],"slug":"object-detection-with-yol-ov5-a-step-by-step-guide","readTime":5,"img":"https://images.pexels.com/photos/574070/pexels-photo-574070.jpeg","author":{"data":{"id":1,"attributes":{"username":"vishwesh","email":"vishwesh.singh1991@gmail.com","provider":"local","confirmed":false,"blocked":false,"createdAt":"2023-04-30T11:12:14.071Z","updatedAt":"2023-04-30T11:12:14.071Z"}}}},"__N_SSG":true},"page":"/python/[python]","query":{"python":"object-detection-with-yol-ov5-a-step-by-step-guide"},"buildId":"VEuKj9rENWBlSP1WmJ69e","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>